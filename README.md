# ReinforcementLearning_CartPole

![ReinforcementLearning_Sanjay Krishnan Venugopal](https://github.com/iamsanjaykrishnan/ReinforcementLearning_CartPole/blob/master/SanjayReinforcementLearning.gif)
100 episodes of random action were performed initially. Based on this data, the critic network was modelled to predict Q value. The actor network is trained to maximize the predicted Q value.<br />
The model runs A2C learning. <br />
Result <br />
Trial 1 : Episode finished after 53 timesteps<br />
Trial 2 : Episode finished after 63 timesteps<br />
Trial 3 : Episode finished after 88 timesteps<br />
Trial 4 : Episode finished after 11 timesteps<br />
Trial 5 : Episode finished after 37 timesteps<br />
Trial 6 : Episode finished after 200 timesteps<br />
Trial 7 : Episode finished after 200 timesteps<br />
Trial 8 : Episode finished after 200 timesteps<br />
Trial 9 : Episode finished after 200 timesteps<br />
Trial 10 : Episode finished after 118 timesteps<br />
Trial 11 : Episode finished after 75 timesteps<br />
Trial 12 : Episode finished after 200 timesteps<br />
Trial 13 : Episode finished after 89 timesteps<br />
Trial 14 : Episode finished after 99 timesteps<br />
Trial 15 : Episode finished after 100 timesteps<br />
Trial 16 : Episode finished after 91 timesteps<br />
Trial 17 : Episode finished after 97 timesteps<br />
Trial 18 : Episode finished after 89 timesteps<br />
Trial 19 : Episode finished after 97 timesteps<br />
Trial 20 : Episode finished after 107 timesteps<br />

# ReinforcementLearning_CartPole_Architecture -> Actor Critic or GAN
![ReinforcementLearning_A2C](https://github.com/iamsanjaykrishnan/ReinforcementLearning_CartPole/blob/master/NetworkArchitecture.png)
